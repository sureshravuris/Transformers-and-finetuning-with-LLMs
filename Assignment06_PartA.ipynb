{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schumbar/CMPE297/blob/main/assignment_06/ShawnChumbar_Assignment06_PartA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 06 Part A: Implement NanoGPT\n",
        "\n",
        "## Assignment Description\n",
        "\n",
        "This portion of the assignment involves implementing NanoGPT from scratch using PyTorch. After building the implementation, you will train the model using a book of your choice, ensuring it differs from Shakespeare's works, which are used in the original NanoGPT setup.\n",
        "\n",
        "The final submission should include several deliverables: a Colab notebook demonstrating the code, a Medium article that explains each section of the code in depth, the input and output files used in training, and a short, 10-minute presentation that walks through the implementation step by step. Utilizing the GPT-4 Code Interpreter plugin for assistance is encouraged to streamline the coding process and debugging.\n",
        "\n",
        "### List of Deliverables:\n",
        "1. Colab Notebook Demonstrating the code\n",
        "2. Medium article that explains each section of the code in depth\n",
        "3. Input files\n",
        "4. Output files\n",
        "5. 10-minute video presentation that walks through implementation step by step.\n",
        "\n",
        "### References:\n",
        "1. [NanoGPT Colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n",
        "2. [YouTube Tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=18s)\n",
        "3. [Class Presentation](https://docs.google.com/presentation/d/1fk8QlODYkBTTH4ftw8M7Sw_tmhJa8KB97s7dYP6s4mI/edit#slide=id.g24535d0c6d4_0_178)\n",
        "4. ChatGPT Code Interpretter plugin\n"
      ],
      "metadata": {
        "id": "szsoZo0zQVlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "dvpH6-X6rGrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the nanoGPT repository\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "%cd nanoGPT\n",
        "\n",
        "# Install required packages\n",
        "!pip install numpy transformers datasets tiktoken wandb tqdm\n",
        "\n",
        "# Upgrade PyTorch to version 2.0.1 with CUDA 11.8 support\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb96cd21-62b6-47e1-f6c6-7165aa291355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Total 682 (delta 0), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (682/682), 952.47 KiB | 32.84 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n",
            "/content/nanoGPT\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, tiktoken, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 tiktoken-0.8.0 xxhash-3.5.0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m973.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.1.4)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (11.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89991 sha256=d4b17cde348f469babcde85d23cbaf23a44a3241501ad80ef68843083a583cb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Successfully installed lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Torch Version"
      ],
      "metadata": {
        "id": "OWPg8vNnrOQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV-ioiEGYxHC",
        "outputId": "b14c1ab7-ef69-4898-ce07-766c9a5c4b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Shakespeare's Works and Create Data Directory"
      ],
      "metadata": {
        "id": "RcxxLWAkrP7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Shakespeare's works\n",
        "!wget https://www.gutenberg.org/files/100/100-0.txt -O shakespeare.txt\n",
        "\n",
        "# Create a directory for the data\n",
        "!mkdir data/shakespeare\n",
        "\n",
        "# Move the text file to the data directory\n",
        "!mv shakespeare.txt data/shakespeare/input.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNPdsT4OSAVx",
        "outputId": "58dbc66d-960c-4353-990b-302b1cd0cfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-18 22:29:16--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618733 (5.4M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   5.36M  2.66MB/s    in 2.0s    \n",
            "\n",
            "2024-11-18 22:29:19 (2.66 MB/s) - ‘shakespeare.txt’ saved [5618733/5618733]\n",
            "\n",
            "mkdir: cannot create directory ‘data/shakespeare’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "r5PSEyuorTa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prepare.py script\n",
        "%%writefile data/shakespeare/prepare.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Define input and output paths\n",
        "input_file_path = 'data/shakespeare/input.txt'\n",
        "train_output_file_path = 'data/shakespeare/train.bin'\n",
        "val_output_file_path = 'data/shakespeare/val.bin'\n",
        "\n",
        "# Read the input text\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Get all unique characters\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Unique characters: {vocab_size}\")\n",
        "\n",
        "# Create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# Encode the entire text data\n",
        "data_size = len(data)\n",
        "print(f\"Data has {data_size} characters.\")\n",
        "\n",
        "# Convert data to integers\n",
        "encoded_data = np.array([stoi[c] for c in data], dtype=np.uint16)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "n = int(0.9 * len(encoded_data))\n",
        "train_data = encoded_data[:n]\n",
        "val_data = encoded_data[n:]\n",
        "\n",
        "# Save the data to .bin files\n",
        "train_data.tofile(train_output_file_path)\n",
        "val_data.tofile(val_output_file_path)\n",
        "\n",
        "# Save the mapping for decoding\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open('data/shakespeare/meta.pkl', 'wb') as f:\n",
        "    pickle.dump(meta, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ijcyeLgSATd",
        "outputId": "b4aadd7d-1ce1-406f-fbc6-45cccb8da2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data/shakespeare/prepare.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the data preparation script\n",
        "!python data/shakespeare/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAB-fsglSARL",
        "outputId": "3f5714fa-09af-4e8b-fbce-2fee7f17b5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters: 100\n",
            "Data has 5359439 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the shakespeare.py config file\n",
        "%%writefile config/shakespeare.py\n",
        "\n",
        "# Configuration for training nanoGPT on Shakespeare's works\n",
        "\n",
        "out_dir = 'out-shakespeare'\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "log_interval = 100\n",
        "\n",
        "always_save_checkpoint = False\n",
        "\n",
        "wandb_log = False  # Set to True if using Weights & Biases\n",
        "wandb_project = 'shakespeare'\n",
        "wandb_run_name = 'mini-gpt'\n",
        "\n",
        "dataset = 'shakespeare'\n",
        "batch_size = 64\n",
        "block_size = 256  # Context length\n",
        "\n",
        "# Model configuration\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5000\n",
        "lr_decay_iters = 5000  # Make equal to max_iters\n",
        "min_lr = 1e-4\n",
        "beta2 = 0.99\n",
        "\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkWwKY5jSAOz",
        "outputId": "372c1e73-b99f-4a68-cca6-9eb17c5dec56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config/shakespeare.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if CUDA is Available"
      ],
      "metadata": {
        "id": "u0_clRpmrdWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"CUDA is available. GPU device name: {gpu_name}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. No GPU detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCmNoj1JZyUQ",
        "outputId": "b91871bd-327e-443f-ed08-e14e3d557c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. GPU device name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "1MzSkvLUrfoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "!python train.py config/shakespeare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3tl-VWGSAMn",
        "outputId": "e039f60a-ecb4-48d6-d7cc-28f616280464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/shakespeare.py:\n",
            "\n",
            "# Configuration for training nanoGPT on Shakespeare's works\n",
            "\n",
            "out_dir = 'out-shakespeare'\n",
            "eval_interval = 500\n",
            "eval_iters = 200\n",
            "log_interval = 100\n",
            "\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False  # Set to True if using Weights & Biases\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "batch_size = 64\n",
            "block_size = 256  # Context length\n",
            "\n",
            "# Model configuration\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000  # Make equal to max_iters\n",
            "min_lr = 1e-4\n",
            "beta2 = 0.99\n",
            "\n",
            "warmup_iters = 100\n",
            "\n",
            "tokens per iteration will be: 655,360\n",
            "found vocab_size = 100 (inside data/shakespeare/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.66M\n",
            "num decayed parameter tensors: 26, with 10,753,536 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.6115, val loss 4.6009\n",
            "[2024-11-18 22:29:38,800] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:39,307] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:40,189] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:40,493] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:40,949] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:41,250] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:41,707] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:42,007] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:42,453] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:42,907] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:43,362] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2024-11-18 22:29:43,663] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.6308, time 21692.12ms, mfu -100.00%\n",
            "iter 100: loss 2.4997, time 893.04ms, mfu 16.71%\n",
            "iter 200: loss 2.0246, time 900.96ms, mfu 16.69%\n",
            "iter 300: loss 1.6570, time 899.31ms, mfu 16.68%\n",
            "iter 400: loss 1.4684, time 895.68ms, mfu 16.68%\n",
            "step 500: train loss 1.2701, val loss 1.4630\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 500: loss 1.3204, time 7198.56ms, mfu 15.22%\n",
            "iter 600: loss 1.3048, time 929.46ms, mfu 15.30%\n",
            "iter 700: loss 1.2122, time 917.05ms, mfu 15.40%\n",
            "iter 800: loss 1.1784, time 903.99ms, mfu 15.51%\n",
            "iter 900: loss 1.1693, time 902.28ms, mfu 15.61%\n",
            "step 1000: train loss 1.0517, val loss 1.3474\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 1000: loss 1.1502, time 4313.46ms, mfu 14.40%\n",
            "iter 1100: loss 1.1157, time 908.82ms, mfu 14.60%\n",
            "iter 1200: loss 1.1073, time 907.76ms, mfu 14.78%\n",
            "iter 1300: loss 1.0460, time 897.15ms, mfu 14.97%\n",
            "iter 1400: loss 1.0436, time 911.81ms, mfu 15.11%\n",
            "step 1500: train loss 0.9231, val loss 1.3518\n",
            "iter 1500: loss 1.0272, time 4013.84ms, mfu 13.97%\n",
            "iter 1600: loss 1.0094, time 903.10ms, mfu 14.23%\n",
            "iter 1700: loss 0.9933, time 919.60ms, mfu 14.43%\n",
            "iter 1800: loss 0.9963, time 908.53ms, mfu 14.63%\n",
            "iter 1900: loss 0.9814, time 893.54ms, mfu 14.83%\n",
            "step 2000: train loss 0.8307, val loss 1.3742\n",
            "iter 2000: loss 1.0020, time 4017.72ms, mfu 13.72%\n",
            "iter 2100: loss 0.9584, time 905.38ms, mfu 14.00%\n",
            "iter 2200: loss 0.9765, time 893.55ms, mfu 14.27%\n",
            "iter 2300: loss 0.9403, time 906.65ms, mfu 14.49%\n",
            "iter 2400: loss 0.9351, time 903.65ms, mfu 14.69%\n",
            "step 2500: train loss 0.7598, val loss 1.4128\n",
            "iter 2500: loss 0.9479, time 3994.17ms, mfu 13.59%\n",
            "iter 2600: loss 0.9013, time 900.86ms, mfu 13.89%\n",
            "iter 2700: loss 0.8994, time 894.77ms, mfu 14.17%\n",
            "iter 2800: loss 0.9085, time 905.41ms, mfu 14.40%\n",
            "iter 2900: loss 0.9001, time 893.79ms, mfu 14.63%\n",
            "step 3000: train loss 0.7084, val loss 1.4434\n",
            "iter 3000: loss 0.8846, time 4014.85ms, mfu 13.54%\n",
            "iter 3100: loss 0.8831, time 892.87ms, mfu 13.86%\n",
            "iter 3200: loss 0.8769, time 919.02ms, mfu 14.09%\n",
            "iter 3300: loss 0.8870, time 917.92ms, mfu 14.31%\n",
            "iter 3400: loss 0.8635, time 889.35ms, mfu 14.56%\n",
            "step 3500: train loss 0.6705, val loss 1.4706\n",
            "iter 3500: loss 0.8817, time 3981.11ms, mfu 13.48%\n",
            "iter 3600: loss 0.8882, time 906.61ms, mfu 13.77%\n",
            "iter 3700: loss 0.8470, time 894.03ms, mfu 14.07%\n",
            "iter 3800: loss 0.8534, time 929.74ms, mfu 14.26%\n",
            "iter 3900: loss 0.8459, time 909.17ms, mfu 14.48%\n",
            "step 4000: train loss 0.6387, val loss 1.4888\n",
            "iter 4000: loss 0.8696, time 4020.63ms, mfu 13.40%\n",
            "iter 4100: loss 0.8286, time 899.53ms, mfu 13.72%\n",
            "iter 4200: loss 0.8417, time 908.88ms, mfu 13.99%\n",
            "iter 4300: loss 0.8747, time 908.77ms, mfu 14.23%\n",
            "iter 4400: loss 0.8452, time 898.97ms, mfu 14.47%\n",
            "step 4500: train loss 0.6222, val loss 1.5029\n",
            "iter 4500: loss 0.8300, time 4019.63ms, mfu 13.39%\n",
            "iter 4600: loss 0.8115, time 903.15ms, mfu 13.71%\n",
            "iter 4700: loss 0.8032, time 943.08ms, mfu 13.92%\n",
            "iter 4800: loss 0.8186, time 908.81ms, mfu 14.17%\n",
            "iter 4900: loss 0.8233, time 905.47ms, mfu 14.40%\n",
            "step 5000: train loss 0.6085, val loss 1.5135\n",
            "iter 5000: loss 0.8263, time 4025.69ms, mfu 13.33%\n"
          ]
        }
      ]
    }
  ]
}